#!/usr/bin/env python3
"""
AWS Macieç»“æœåˆ†æå·¥å…·
ç”¨äºæ·±åº¦åˆ†æMacieæ‰«æç»“æœå¹¶ç”Ÿæˆè¯¦ç»†æŠ¥å‘Š
"""

import json
import boto3
import argparse
from datetime import datetime
from pathlib import Path
from typing import Dict, List, Any
import logging

logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

class MacieResultsAnalyzer:
    def __init__(self, region='us-east-1', profile=None):
        """åˆå§‹åŒ–AWSå®¢æˆ·ç«¯"""
        session = boto3.Session(profile_name=profile) if profile else boto3.Session()
        self.macie_client = session.client('macie2', region_name=region)
        self.s3_client = session.client('s3', region_name=region)
        self.region = region
        
    def get_job_findings(self, job_id: str) -> List[Dict]:
        """è·å–æŒ‡å®šä½œä¸šçš„æ‰€æœ‰å‘ç°"""
        logger.info(f"è·å–ä½œä¸šå‘ç°: {job_id}")
        
        findings = []
        paginator = self.macie_client.get_paginator('list_findings')
        
        try:
            page_iterator = paginator.paginate(
                findingCriteria={
                    'criterion': {
                        'classificationDetails.jobId': {
                            'eq': [job_id]
                        }
                    }
                }
            )
            
            for page in page_iterator:
                finding_ids = page.get('findingIds', [])
                
                if finding_ids:
                    # æ‰¹é‡è·å–å‘ç°è¯¦æƒ…
                    findings_response = self.macie_client.get_findings(findingIds=finding_ids)
                    findings.extend(findings_response.get('findings', []))
            
            logger.info(f"è·å–åˆ° {len(findings)} ä¸ªå‘ç°")
            return findings
            
        except Exception as e:
            logger.error(f"è·å–å‘ç°å¤±è´¥: {e}")
            return []
    
    def analyze_sensitive_data_types(self, findings: List[Dict]) -> Dict:
        """åˆ†ææ•æ„Ÿæ•°æ®ç±»å‹åˆ†å¸ƒ"""
        data_types = {}
        severity_counts = {'HIGH': 0, 'MEDIUM': 0, 'LOW': 0}
        
        for finding in findings:
            # åˆ†æä¸¥é‡ç¨‹åº¦
            severity = finding.get('severity', {}).get('description', 'UNKNOWN')
            if severity in severity_counts:
                severity_counts[severity] += 1
            
            # åˆ†ææ•æ„Ÿæ•°æ®ç±»å‹
            classification_details = finding.get('classificationDetails', {})
            result = classification_details.get('result', {})
            
            if 'sensitiveData' in result:
                for sensitive_data in result['sensitiveData']:
                    category = sensitive_data.get('category', 'UNKNOWN')
                    detections = sensitive_data.get('detections', [])
                    
                    if category not in data_types:
                        data_types[category] = {
                            'count': 0,
                            'types': {},
                            'total_occurrences': 0
                        }
                    
                    data_types[category]['count'] += 1
                    
                    for detection in detections:
                        detection_type = detection.get('type', 'UNKNOWN')
                        occurrences = detection.get('count', 1)
                        
                        if detection_type not in data_types[category]['types']:
                            data_types[category]['types'][detection_type] = 0
                        
                        data_types[category]['types'][detection_type] += occurrences
                        data_types[category]['total_occurrences'] += occurrences
        
        return {
            'data_types': data_types,
            'severity_distribution': severity_counts,
            'total_findings': len(findings)
        }
    
    def analyze_file_distribution(self, findings: List[Dict]) -> Dict:
        """åˆ†ææ–‡ä»¶åˆ†å¸ƒæƒ…å†µ"""
        file_stats = {}
        
        for finding in findings:
            resources = finding.get('resources', [])
            
            for resource in resources:
                if resource.get('resourcesAffected', {}).get('s3Object'):
                    s3_obj = resource['resourcesAffected']['s3Object']
                    bucket_name = s3_obj.get('bucketName', 'UNKNOWN')
                    key = s3_obj.get('key', 'UNKNOWN')
                    
                    if bucket_name not in file_stats:
                        file_stats[bucket_name] = {}
                    
                    if key not in file_stats[bucket_name]:
                        file_stats[bucket_name][key] = {
                            'findings_count': 0,
                            'size': s3_obj.get('size', 0),
                            'last_modified': s3_obj.get('lastModified'),
                            'storage_class': s3_obj.get('storageClass', 'UNKNOWN')
                        }
                    
                    file_stats[bucket_name][key]['findings_count'] += 1
        
        return file_stats
    
    def generate_detailed_report(self, job_id: str, output_file: str = None) -> Dict:
        """ç”Ÿæˆè¯¦ç»†çš„åˆ†ææŠ¥å‘Š"""
        logger.info(f"ç”Ÿæˆè¯¦ç»†æŠ¥å‘Š: {job_id}")
        
        # è·å–ä½œä¸šä¿¡æ¯
        try:
            job_info = self.macie_client.describe_classification_job(jobId=job_id)
        except Exception as e:
            logger.error(f"è·å–ä½œä¸šä¿¡æ¯å¤±è´¥: {e}")
            return {}
        
        # è·å–å‘ç°
        findings = self.get_job_findings(job_id)
        
        # åˆ†ææ•°æ®
        sensitive_analysis = self.analyze_sensitive_data_types(findings)
        file_analysis = self.analyze_file_distribution(findings)
        
        # æ„å»ºæŠ¥å‘Š
        report = {
            'report_metadata': {
                'generated_at': datetime.now().isoformat(),
                'job_id': job_id,
                'analyzer_version': '1.0.0'
            },
            'job_summary': {
                'name': job_info.get('name'),
                'status': job_info.get('jobStatus'),
                'created_at': str(job_info.get('createdAt')),
                'completed_at': str(job_info.get('lastRunTime')),
                'statistics': job_info.get('statistics', {})
            },
            'findings_analysis': sensitive_analysis,
            'file_distribution': file_analysis,
            'detailed_findings': findings[:20] if len(findings) > 20 else findings,  # é™åˆ¶è¯¦ç»†å‘ç°æ•°é‡
            'recommendations': self.generate_recommendations(sensitive_analysis, file_analysis)
        }
        
        # ä¿å­˜æŠ¥å‘Š
        if not output_file:
            timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
            output_file = f"detailed_macie_analysis_{timestamp}.json"
        
        with open(output_file, 'w', encoding='utf-8') as f:
            json.dump(report, f, indent=2, default=str, ensure_ascii=False)
        
        logger.info(f"è¯¦ç»†æŠ¥å‘Šå·²ä¿å­˜: {output_file}")
        
        # ç”Ÿæˆäººç±»å¯è¯»çš„æ‘˜è¦
        self.generate_human_readable_summary(report, output_file.replace('.json', '_summary.txt'))
        
        return report
    
    def generate_recommendations(self, sensitive_analysis: Dict, file_analysis: Dict) -> List[str]:
        """ç”Ÿæˆå®‰å…¨å»ºè®®"""
        recommendations = []
        
        data_types = sensitive_analysis.get('data_types', {})
        severity_dist = sensitive_analysis.get('severity_distribution', {})
        
        # åŸºäºå‘ç°çš„æ•æ„Ÿæ•°æ®ç±»å‹ç»™å‡ºå»ºè®®
        if 'PII' in data_types:
            recommendations.append("å‘ç°ä¸ªäººèº«ä»½ä¿¡æ¯(PII)ï¼Œå»ºè®®å®æ–½æ•°æ®è„±æ•æˆ–åŠ å¯†å­˜å‚¨")
        
        if 'FINANCIAL_INFORMATION' in data_types:
            recommendations.append("å‘ç°è´¢åŠ¡ä¿¡æ¯ï¼Œå»ºè®®åŠ å¼ºè®¿é—®æ§åˆ¶å’Œå®¡è®¡æ—¥å¿—")
        
        if 'CREDENTIALS' in data_types:
            recommendations.append("å‘ç°å‡­è¯ä¿¡æ¯ï¼Œå»ºè®®ç«‹å³è½®æ¢ç›¸å…³å¯†é’¥å¹¶åŠ å¼ºå¯†é’¥ç®¡ç†")
        
        # åŸºäºä¸¥é‡ç¨‹åº¦ç»™å‡ºå»ºè®®
        if severity_dist.get('HIGH', 0) > 0:
            recommendations.append(f"å‘ç° {severity_dist['HIGH']} ä¸ªé«˜é£é™©é¡¹ï¼Œå»ºè®®ä¼˜å…ˆå¤„ç†")
        
        if severity_dist.get('MEDIUM', 0) > 10:
            recommendations.append("ä¸­ç­‰é£é™©é¡¹è¾ƒå¤šï¼Œå»ºè®®åˆ¶å®šæ‰¹é‡å¤„ç†è®¡åˆ’")
        
        # åŸºäºæ–‡ä»¶åˆ†å¸ƒç»™å‡ºå»ºè®®
        total_files = sum(len(files) for files in file_analysis.values())
        if total_files > 50:
            recommendations.append("æ¶‰åŠæ–‡ä»¶è¾ƒå¤šï¼Œå»ºè®®å®æ–½è‡ªåŠ¨åŒ–æ•°æ®åˆ†ç±»å’Œä¿æŠ¤ç­–ç•¥")
        
        if not recommendations:
            recommendations.append("æœªå‘ç°æ˜æ˜¾çš„æ•æ„Ÿæ•°æ®ï¼Œå»ºè®®å®šæœŸé‡æ–°æ‰«æä»¥ç¡®ä¿æŒç»­åˆè§„")
        
        return recommendations
    
    def generate_human_readable_summary(self, report: Dict, output_file: str):
        """ç”Ÿæˆäººç±»å¯è¯»çš„æ‘˜è¦æŠ¥å‘Š"""
        with open(output_file, 'w', encoding='utf-8') as f:
            f.write("LOKI CHUNK æ•æ„Ÿæ•°æ®åˆ†ææŠ¥å‘Š\n")
            f.write("=" * 50 + "\n\n")
            
            # ä½œä¸šæ‘˜è¦
            job_summary = report['job_summary']
            f.write(f"ä½œä¸šåç§°: {job_summary.get('name')}\n")
            f.write(f"ä½œä¸šçŠ¶æ€: {job_summary.get('status')}\n")
            f.write(f"åˆ›å»ºæ—¶é—´: {job_summary.get('created_at')}\n")
            f.write(f"å®Œæˆæ—¶é—´: {job_summary.get('completed_at')}\n\n")
            
            # ç»Ÿè®¡ä¿¡æ¯
            stats = job_summary.get('statistics', {})
            f.write("æ‰«æç»Ÿè®¡:\n")
            f.write(f"  å¤„ç†å¯¹è±¡æ•°: {stats.get('approximateNumberOfObjectsToProcess', 0)}\n")
            f.write(f"  å·²å¤„ç†å¯¹è±¡: {stats.get('approximateNumberOfObjectsProcessed', 0)}\n\n")
            
            # å‘ç°æ‘˜è¦
            findings_analysis = report['findings_analysis']
            f.write(f"å‘ç°æ‘˜è¦:\n")
            f.write(f"  æ€»å‘ç°æ•°: {findings_analysis.get('total_findings', 0)}\n")
            
            severity_dist = findings_analysis.get('severity_distribution', {})
            f.write(f"  é«˜é£é™©: {severity_dist.get('HIGH', 0)}\n")
            f.write(f"  ä¸­é£é™©: {severity_dist.get('MEDIUM', 0)}\n")
            f.write(f"  ä½é£é™©: {severity_dist.get('LOW', 0)}\n\n")
            
            # æ•æ„Ÿæ•°æ®ç±»å‹
            data_types = findings_analysis.get('data_types', {})
            if data_types:
                f.write("å‘ç°çš„æ•æ„Ÿæ•°æ®ç±»å‹:\n")
                for category, info in data_types.items():
                    f.write(f"  {category}: {info['count']} ä¸ªå‘ç°, {info['total_occurrences']} æ¬¡å‡ºç°\n")
                    for data_type, count in info['types'].items():
                        f.write(f"    - {data_type}: {count} æ¬¡\n")
                f.write("\n")
            
            # å»ºè®®
            recommendations = report.get('recommendations', [])
            if recommendations:
                f.write("å®‰å…¨å»ºè®®:\n")
                for i, rec in enumerate(recommendations, 1):
                    f.write(f"  {i}. {rec}\n")
        
        logger.info(f"æ‘˜è¦æŠ¥å‘Šå·²ä¿å­˜: {output_file}")

def main():
    parser = argparse.ArgumentParser(description='AWS Macieç»“æœåˆ†æå·¥å…·')
    parser.add_argument('--job-id', required=True, help='Macieä½œä¸šID')
    parser.add_argument('--output', help='è¾“å‡ºæ–‡ä»¶å')
    parser.add_argument('--region', default='us-east-1', help='AWSåŒºåŸŸ')
    parser.add_argument('--profile', help='AWSé…ç½®æ–‡ä»¶åç§°')
    
    args = parser.parse_args()
    
    try:
        analyzer = MacieResultsAnalyzer(region=args.region, profile=args.profile)
        report = analyzer.generate_detailed_report(args.job_id, args.output)
        
        if report:
            print("âœ… åˆ†æå®Œæˆï¼")
            print(f"ğŸ“Š æ€»å‘ç°æ•°: {report['findings_analysis']['total_findings']}")
            print(f"ğŸ“„ è¯¦ç»†æŠ¥å‘Š: {args.output or 'detailed_macie_analysis_*.json'}")
            return 0
        else:
            print("âŒ åˆ†æå¤±è´¥ï¼")
            return 1
            
    except Exception as e:
        logger.error(f"åˆ†æå¼‚å¸¸: {e}")
        return 1

if __name__ == '__main__':
    exit(main())
