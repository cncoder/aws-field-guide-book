#!/usr/bin/env python3
"""
Loki Chunk åˆ° AWS Macie å®Œæ•´åˆ†æç®¡é“
å·¥ä½œæµç¨‹:
1. å°†Loki chunkæ–‡ä»¶è½¬æ¢ä¸ºæ–‡æœ¬æ ¼å¼
2. æŒ‰æ—¶é—´æ—¥æœŸåˆ†åŒºä¸Šä¼ åˆ°S3å­˜å‚¨æ¡¶ your-scan-bucket/loki-complete/
3. å¯åŠ¨Macieåˆ†ç±»ä½œä¸š
4. ç­‰å¾…æ‰«æå®Œæˆå¹¶åˆ†æç»“æœ
5. ç»“æœå­˜å‚¨åœ¨ your-results-bucket å­˜å‚¨æ¡¶
"""

import os
import json
import boto3
import time
import subprocess
from datetime import datetime, timezone
from pathlib import Path
import argparse
from typing import Dict, List, Any, Optional
import logging

# é…ç½®æ—¥å¿—
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s',
    handlers=[
        logging.FileHandler('loki_macie_pipeline.log'),
        logging.StreamHandler()
    ]
)
logger = logging.getLogger(__name__)

class LokiMaciePipeline:
    def __init__(self, region=None, profile=None, config_file='config.json'):
        """åˆå§‹åŒ–AWSå®¢æˆ·ç«¯"""
        
        # åŠ è½½é…ç½®æ–‡ä»¶
        self.config = self.load_config(config_file)
        
        # æ£€æŸ¥å¹¶å¼•å¯¼ç”¨æˆ·é…ç½®
        self.interactive_config_setup()
        
        # ä½¿ç”¨å‚æ•°æˆ–é…ç½®æ–‡ä»¶ä¸­çš„è®¾ç½®
        self.region = region or self.config['aws']['region']
        self.profile = profile or self.config['aws']['profile']
        
        # é…ç½®AWSä¼šè¯
        session = boto3.Session(profile_name=self.profile) if self.profile else boto3.Session()
        self.s3_client = session.client('s3', region_name=self.region)
        self.macie_client = session.client('macie2', region_name=self.region)
        self.sts_client = session.client('sts', region_name=self.region)
        
        # S3å­˜å‚¨æ¡¶é…ç½® - ä»é…ç½®æ–‡ä»¶è¯»å–
        self.scan_bucket = self.config['s3']['scan_bucket']
        self.results_bucket = self.config['s3']['results_bucket']
        self.s3_prefix = self.config['s3']['scan_prefix']
        
        # è·å–è´¦æˆ·ä¿¡æ¯
        try:
            self.account_id = self.sts_client.get_caller_identity()['Account']
            logger.info(f"å½“å‰AWSè´¦æˆ·: {self.account_id}")
        except Exception as e:
            logger.error(f"è·å–è´¦æˆ·IDå¤±è´¥: {e}")
            raise
            
        # æ—¶é—´æˆ³ç”¨äºåˆ†åŒº
        self.timestamp = datetime.now(timezone.utc)
        self.date_partition = self.timestamp.strftime('%Y/%m/%d')
        self.job_name = f"loki-analysis-{self.timestamp.strftime('%Y%m%d-%H%M%S')}"
    
    def interactive_config_setup(self):
        """äº¤äº’å¼é…ç½®è®¾ç½® - åŠ å¼ºç‰ˆ"""
        needs_config = False
        config_issues = []
        
        # æ£€æŸ¥æ‰€æœ‰å¯èƒ½çš„é»˜è®¤å€¼å’Œé—®é¢˜
        if self.config['s3']['scan_bucket'] == 'your-macie-scan-bucket':
            needs_config = True
            config_issues.append("æ‰«æå­˜å‚¨æ¡¶ä½¿ç”¨é»˜è®¤å€¼")
        
        if self.config['s3']['results_bucket'] == 'your-macie-results-bucket':
            needs_config = True
            config_issues.append("ç»“æœå­˜å‚¨æ¡¶ä½¿ç”¨é»˜è®¤å€¼")
        
        if not self.config['aws']['region'] or self.config['aws']['region'] == 'your-region':
            needs_config = True
            config_issues.append("AWSåŒºåŸŸæœªè®¾ç½®æˆ–ä½¿ç”¨é»˜è®¤å€¼")
        
        # æ£€æŸ¥å­˜å‚¨æ¡¶åç§°æ˜¯å¦åˆç†ï¼ˆé¿å…æµ‹è¯•åç§°ï¼‰
        if 'test' in self.config['s3']['scan_bucket'].lower() or 'example' in self.config['s3']['scan_bucket'].lower():
            needs_config = True
            config_issues.append("æ‰«æå­˜å‚¨æ¡¶åç§°çœ‹èµ·æ¥åƒæµ‹è¯•å€¼")
        
        if 'test' in self.config['s3']['results_bucket'].lower() or 'example' in self.config['s3']['results_bucket'].lower():
            needs_config = True
            config_issues.append("ç»“æœå­˜å‚¨æ¡¶åç§°çœ‹èµ·æ¥åƒæµ‹è¯•å€¼")
        
        if needs_config:
            print("\n" + "ğŸš¨" * 20)
            print("âš ï¸  é…ç½®æ£€æŸ¥å¤±è´¥ - éœ€è¦ç”¨æˆ·è¾“å…¥")
            print("ğŸš¨" * 20)
            print("\nå‘ç°ä»¥ä¸‹é…ç½®é—®é¢˜:")
            for i, issue in enumerate(config_issues, 1):
                print(f"  {i}. {issue}")
            
            print(f"\nå½“å‰é…ç½®:")
            print(f"  AWSåŒºåŸŸ: {self.config['aws']['region']}")
            print(f"  æ‰«æå­˜å‚¨æ¡¶: {self.config['s3']['scan_bucket']}")
            print(f"  ç»“æœå­˜å‚¨æ¡¶: {self.config['s3']['results_bucket']}")
            
            print("\n" + "="*60)
            print("ğŸ”§ è¯·è¾“å…¥æ­£ç¡®çš„é…ç½®ä¿¡æ¯")
            print("="*60)
            
            # å¼ºåˆ¶ç”¨æˆ·è¾“å…¥ï¼Œä¸å…è®¸è·³è¿‡
            self.force_user_input()
        else:
            print("âœ… é…ç½®æ£€æŸ¥é€šè¿‡")
    
    def force_user_input(self):
        """å¼ºåˆ¶ç”¨æˆ·è¾“å…¥é…ç½®ï¼Œä¸å…è®¸è·³è¿‡"""
        max_attempts = 3
        
        # AWSåŒºåŸŸé…ç½®
        for attempt in range(max_attempts):
            region_input = input(f"\n[å¿…å¡«] è¯·è¾“å…¥AWSåŒºåŸŸ (å¦‚: us-east-1, ap-northeast-1): ").strip()
            if region_input and len(region_input) > 5:  # åŸºæœ¬éªŒè¯
                self.config['aws']['region'] = region_input
                break
            else:
                print(f"âŒ AWSåŒºåŸŸä¸èƒ½ä¸ºç©ºä¸”æ ¼å¼è¦æ­£ç¡® (å‰©ä½™å°è¯•: {max_attempts - attempt - 1})")
                if attempt == max_attempts - 1:
                    raise ValueError("AWSåŒºåŸŸé…ç½®å¤±è´¥ï¼Œç¨‹åºé€€å‡º")
        
        # æ‰«æå­˜å‚¨æ¡¶é…ç½®
        for attempt in range(max_attempts):
            scan_bucket = input(f"\n[å¿…å¡«] è¯·è¾“å…¥æ‰«æå­˜å‚¨æ¡¶åç§° (ç”¨äºå­˜å‚¨å¾…åˆ†ææ–‡ä»¶): ").strip()
            if scan_bucket and len(scan_bucket) >= 3 and scan_bucket != 'your-macie-scan-bucket':
                # éªŒè¯å­˜å‚¨æ¡¶åç§°æ ¼å¼
                if self.validate_bucket_name(scan_bucket):
                    self.config['s3']['scan_bucket'] = scan_bucket
                    break
                else:
                    print("âŒ å­˜å‚¨æ¡¶åç§°æ ¼å¼ä¸æ­£ç¡®")
            else:
                print(f"âŒ æ‰«æå­˜å‚¨æ¡¶åç§°ä¸èƒ½ä¸ºç©ºæˆ–ä½¿ç”¨é»˜è®¤å€¼ (å‰©ä½™å°è¯•: {max_attempts - attempt - 1})")
                if attempt == max_attempts - 1:
                    raise ValueError("æ‰«æå­˜å‚¨æ¡¶é…ç½®å¤±è´¥ï¼Œç¨‹åºé€€å‡º")
        
        # ç»“æœå­˜å‚¨æ¡¶é…ç½®
        for attempt in range(max_attempts):
            results_bucket = input(f"\n[å¿…å¡«] è¯·è¾“å…¥ç»“æœå­˜å‚¨æ¡¶åç§° (ç”¨äºå­˜å‚¨åˆ†æç»“æœ): ").strip()
            if results_bucket and len(results_bucket) >= 3 and results_bucket != 'your-macie-results-bucket':
                # éªŒè¯å­˜å‚¨æ¡¶åç§°æ ¼å¼
                if self.validate_bucket_name(results_bucket):
                    # ç¡®ä¿ä¸¤ä¸ªå­˜å‚¨æ¡¶ä¸åŒ
                    if results_bucket != self.config['s3']['scan_bucket']:
                        self.config['s3']['results_bucket'] = results_bucket
                        break
                    else:
                        print("âŒ ç»“æœå­˜å‚¨æ¡¶ä¸èƒ½ä¸æ‰«æå­˜å‚¨æ¡¶ç›¸åŒ")
                else:
                    print("âŒ å­˜å‚¨æ¡¶åç§°æ ¼å¼ä¸æ­£ç¡®")
            else:
                print(f"âŒ ç»“æœå­˜å‚¨æ¡¶åç§°ä¸èƒ½ä¸ºç©ºæˆ–ä½¿ç”¨é»˜è®¤å€¼ (å‰©ä½™å°è¯•: {max_attempts - attempt - 1})")
                if attempt == max_attempts - 1:
                    raise ValueError("ç»“æœå­˜å‚¨æ¡¶é…ç½®å¤±è´¥ï¼Œç¨‹åºé€€å‡º")
        
        # å¯é€‰ï¼šæœ€å¤§ç­‰å¾…æ—¶é—´
        wait_input = input(f"\n[å¯é€‰] æœ€å¤§ç­‰å¾…æ—¶é—´(åˆ†é’Ÿ) (å½“å‰: {self.config['macie']['max_wait_minutes']}, å›è½¦è·³è¿‡): ").strip()
        if wait_input and wait_input.isdigit() and 1 <= int(wait_input) <= 300:
            self.config['macie']['max_wait_minutes'] = int(wait_input)
        
        # ç¡®è®¤é…ç½®
        print("\n" + "="*60)
        print("ğŸ“‹ è¯·ç¡®è®¤æ‚¨çš„é…ç½®:")
        print("="*60)
        print(f"AWSåŒºåŸŸ: {self.config['aws']['region']}")
        print(f"æ‰«æå­˜å‚¨æ¡¶: {self.config['s3']['scan_bucket']}")
        print(f"ç»“æœå­˜å‚¨æ¡¶: {self.config['s3']['results_bucket']}")
        print(f"æœ€å¤§ç­‰å¾…æ—¶é—´: {self.config['macie']['max_wait_minutes']} åˆ†é’Ÿ")
        
        confirm = input("\nç¡®è®¤ä»¥ä¸Šé…ç½®æ­£ç¡®å—? (è¾“å…¥ 'yes' ç»§ç»­): ").strip().lower()
        if confirm != 'yes':
            print("âŒ ç”¨æˆ·å–æ¶ˆé…ç½®ï¼Œç¨‹åºé€€å‡º")
            raise ValueError("ç”¨æˆ·å–æ¶ˆé…ç½®")
        
        # ä¿å­˜é…ç½®
        self.save_config()
        print("\nâœ… é…ç½®å·²ç¡®è®¤å¹¶ä¿å­˜")
    
    def validate_bucket_name(self, bucket_name):
        """éªŒè¯S3å­˜å‚¨æ¡¶åç§°æ ¼å¼"""
        import re
        
        # S3å­˜å‚¨æ¡¶å‘½åè§„åˆ™
        if len(bucket_name) < 3 or len(bucket_name) > 63:
            return False
        
        # åªèƒ½åŒ…å«å°å†™å­—æ¯ã€æ•°å­—å’Œè¿å­—ç¬¦
        if not re.match(r'^[a-z0-9.-]+$', bucket_name):
            return False
        
        # ä¸èƒ½ä»¥è¿å­—ç¬¦å¼€å¤´æˆ–ç»“å°¾
        if bucket_name.startswith('-') or bucket_name.endswith('-'):
            return False
        
        # ä¸èƒ½åŒ…å«è¿ç»­çš„ç‚¹
        if '..' in bucket_name:
            return False
        
        return True
    
    def save_config(self):
        """ä¿å­˜é…ç½®åˆ°æ–‡ä»¶"""
        try:
            with open('config.json', 'w', encoding='utf-8') as f:
                json.dump(self.config, f, indent=2, ensure_ascii=False)
        except Exception as e:
            logger.error(f"ä¿å­˜é…ç½®æ–‡ä»¶å¤±è´¥: {e}")
            raise
    
    def load_config(self, config_file: str) -> Dict:
        """åŠ è½½é…ç½®æ–‡ä»¶"""
        try:
            with open(config_file, 'r', encoding='utf-8') as f:
                config = json.load(f)
            logger.info(f"é…ç½®æ–‡ä»¶åŠ è½½æˆåŠŸ: {config_file}")
            return config
        except FileNotFoundError:
            logger.error(f"é…ç½®æ–‡ä»¶ä¸å­˜åœ¨: {config_file}")
            raise
        except json.JSONDecodeError as e:
            logger.error(f"é…ç½®æ–‡ä»¶æ ¼å¼é”™è¯¯: {e}")
            raise
        
    def extract_loki_chunks_to_text(self, chunk_dir: str, output_dir: str) -> List[str]:
        """
        å°†Loki chunkæ–‡ä»¶è½¬æ¢ä¸ºæ–‡æœ¬æ ¼å¼
        """
        logger.info(f"å¼€å§‹æå–Loki chunkæ–‡ä»¶: {chunk_dir}")
        
        chunk_path = Path(chunk_dir)
        output_path = Path(output_dir)
        output_path.mkdir(exist_ok=True)
        
        text_files = []
        
        # è·å–æ‰€æœ‰chunkæ–‡ä»¶
        chunk_files = list(chunk_path.glob('*'))
        chunk_files = [f for f in chunk_files if f.is_file() and not f.name.startswith('.')]
        
        logger.info(f"æ‰¾åˆ° {len(chunk_files)} ä¸ªchunkæ–‡ä»¶")
        
        for chunk_file in chunk_files:
            try:
                logger.info(f"å¤„ç†æ–‡ä»¶: {chunk_file.name}")
                
                # ä½¿ç”¨chunks-inspectå·¥å…·æå–
                output_file = output_path / f"{chunk_file.name}.txt"
                
                # è¿è¡Œchunks-inspectå‘½ä»¤
                cmd = ['./chunks-inspect', '-l', str(chunk_file)]
                result = subprocess.run(cmd, capture_output=True, text=True, cwd='.')
                
                if result.returncode == 0:
                    # ä¿å­˜æå–çš„æ–‡æœ¬
                    with open(output_file, 'w', encoding='utf-8') as f:
                        f.write(f"# Loki Chunk File: {chunk_file.name}\n")
                        f.write(f"# Extracted at: {self.timestamp.isoformat()}\n")
                        f.write(f"# File size: {chunk_file.stat().st_size} bytes\n")
                        f.write("# " + "="*50 + "\n\n")
                        f.write(result.stdout)
                    
                    text_files.append(str(output_file))
                    logger.info(f"âœ… æˆåŠŸæå–: {output_file.name}")
                else:
                    logger.error(f"âŒ æå–å¤±è´¥ {chunk_file.name}: {result.stderr}")
                    
            except Exception as e:
                logger.error(f"å¤„ç†æ–‡ä»¶ {chunk_file.name} æ—¶å‡ºé”™: {e}")
                continue
        
        logger.info(f"æå–å®Œæˆï¼Œç”Ÿæˆ {len(text_files)} ä¸ªæ–‡æœ¬æ–‡ä»¶")
        return text_files
    
    def upload_to_s3_with_partition(self, text_files: List[str]) -> List[str]:
        """
        æŒ‰æ—¶é—´åˆ†åŒºä¸Šä¼ æ–‡ä»¶åˆ°S3
        """
        logger.info(f"å¼€å§‹ä¸Šä¼ æ–‡ä»¶åˆ°S3å­˜å‚¨æ¡¶: {self.scan_bucket}")
        
        uploaded_keys = []
        
        for text_file in text_files:
            try:
                file_path = Path(text_file)
                
                # æ„å»ºS3é”®åï¼ŒåŒ…å«æ—¶é—´åˆ†åŒº
                s3_key = f"{self.s3_prefix}/{self.date_partition}/{file_path.name}"
                
                # ä¸Šä¼ æ–‡ä»¶
                self.s3_client.upload_file(
                    str(file_path),
                    self.scan_bucket,
                    s3_key,
                    ExtraArgs={
                        'Metadata': {
                            'source': 'loki-chunk',
                            'extraction-time': self.timestamp.isoformat(),
                            'pipeline-job': self.job_name
                        }
                    }
                )
                
                uploaded_keys.append(s3_key)
                logger.info(f"âœ… ä¸Šä¼ æˆåŠŸ: s3://{self.scan_bucket}/{s3_key}")
                
            except Exception as e:
                logger.error(f"ä¸Šä¼ æ–‡ä»¶ {text_file} å¤±è´¥: {e}")
                continue
        
        logger.info(f"ä¸Šä¼ å®Œæˆï¼Œå…±ä¸Šä¼  {len(uploaded_keys)} ä¸ªæ–‡ä»¶")
        return uploaded_keys
    
    def ensure_macie_enabled(self):
        """
        ç¡®ä¿MacieæœåŠ¡å·²å¯ç”¨
        """
        try:
            # æ£€æŸ¥MacieçŠ¶æ€
            response = self.macie_client.get_macie_session()
            logger.info("âœ… MacieæœåŠ¡å·²å¯ç”¨")
            return True
        except self.macie_client.exceptions.ResourceNotFoundException:
            logger.info("MacieæœåŠ¡æœªå¯ç”¨ï¼Œæ­£åœ¨å¯ç”¨...")
            try:
                self.macie_client.enable_macie(
                    findingPublishingFrequency='FIFTEEN_MINUTES'
                )
                logger.info("âœ… MacieæœåŠ¡å¯ç”¨æˆåŠŸ")
                return True
            except Exception as e:
                logger.error(f"å¯ç”¨MacieæœåŠ¡å¤±è´¥: {e}")
                return False
        except Exception as e:
            logger.error(f"æ£€æŸ¥MacieçŠ¶æ€å¤±è´¥: {e}")
            return False
    
    def create_macie_job(self, s3_keys: List[str]) -> str:
        """
        åˆ›å»ºMacieåˆ†ç±»ä½œä¸š
        """
        logger.info(f"åˆ›å»ºMacieåˆ†ç±»ä½œä¸š: {self.job_name}")
        
        # æ„å»ºS3ä½œä¸šèŒƒå›´
        s3_job_definition = {
            'bucketDefinitions': [
                {
                    'accountId': self.account_id,
                    'buckets': [self.scan_bucket]
                }
            ],
            'scoping': {
                'includes': {
                    'and': [
                        {
                            'simpleScopeTerm': {
                                'comparator': 'STARTS_WITH',
                                'key': 'OBJECT_KEY',
                                'values': [f"{self.s3_prefix}/{self.date_partition}/"]
                            }
                        }
                    ]
                }
            }
        }
        
        try:
            response = self.macie_client.create_classification_job(
                name=self.job_name,
                description=f"Loki chunkæ–‡ä»¶æ•æ„Ÿæ•°æ®åˆ†æ - {self.timestamp.strftime('%Y-%m-%d %H:%M:%S')}",
                jobType='ONE_TIME',
                s3JobDefinition=s3_job_definition,
                samplingPercentage=100,  # 100%é‡‡æ ·
                tags={
                    'Source': 'loki-chunks',
                    'Pipeline': 'loki-macie-pipeline',
                    'Date': self.date_partition.replace('/', '-')
                }
            )
            
            job_id = response['jobId']
            logger.info(f"âœ… Macieä½œä¸šåˆ›å»ºæˆåŠŸ: {job_id}")
            return job_id
            
        except Exception as e:
            logger.error(f"åˆ›å»ºMacieä½œä¸šå¤±è´¥: {e}")
            raise
    
    def wait_for_job_completion(self, job_id: str, max_wait_minutes: int = 60) -> Dict:
        """
        ç­‰å¾…Macieä½œä¸šå®Œæˆ
        """
        logger.info(f"ç­‰å¾…Macieä½œä¸šå®Œæˆ: {job_id}")
        logger.info(f"æœ€å¤§ç­‰å¾…æ—¶é—´: {max_wait_minutes} åˆ†é’Ÿ")
        
        start_time = time.time()
        max_wait_seconds = max_wait_minutes * 60
        
        while True:
            try:
                response = self.macie_client.describe_classification_job(jobId=job_id)
                job_status = response['jobStatus']
                
                logger.info(f"ä½œä¸šçŠ¶æ€: {job_status}")
                
                if job_status == 'COMPLETE':
                    logger.info("âœ… Macieä½œä¸šå®Œæˆ")
                    return response
                elif job_status in ['CANCELLED', 'USER_PAUSED']:
                    logger.warning(f"âš ï¸ ä½œä¸šçŠ¶æ€å¼‚å¸¸: {job_status}")
                    return response
                elif job_status == 'RUNNING':
                    # æ˜¾ç¤ºè¿›åº¦ä¿¡æ¯
                    if 'statistics' in response:
                        stats = response['statistics']
                        logger.info(f"è¿›åº¦ - å·²å¤„ç†å¯¹è±¡: {stats.get('approximateNumberOfObjectsToProcess', 0)}")
                
                # æ£€æŸ¥è¶…æ—¶
                elapsed = time.time() - start_time
                if elapsed > max_wait_seconds:
                    logger.warning(f"âš ï¸ ç­‰å¾…è¶…æ—¶ ({max_wait_minutes} åˆ†é’Ÿ)")
                    return response
                
                # ç­‰å¾…30ç§’åå†æ¬¡æ£€æŸ¥
                time.sleep(30)
                
            except Exception as e:
                logger.error(f"æ£€æŸ¥ä½œä¸šçŠ¶æ€å¤±è´¥: {e}")
                time.sleep(30)
                continue
    
    def analyze_macie_results(self, job_id: str) -> Dict:
        """
        åˆ†æMacieæ‰«æç»“æœ
        """
        logger.info(f"åˆ†æMacieæ‰«æç»“æœ: {job_id}")
        
        try:
            # è·å–ä½œä¸šç»Ÿè®¡ä¿¡æ¯
            job_response = self.macie_client.describe_classification_job(jobId=job_id)
            
            # è·å–å‘ç°çš„æ•æ„Ÿæ•°æ®
            findings_response = self.macie_client.list_findings(
                findingCriteria={
                    'criterion': {
                        'classificationDetails.jobId': {
                            'eq': [job_id]
                        }
                    }
                },
                maxResults=50
            )
            
            # æ„å»ºåˆ†ææŠ¥å‘Š
            analysis_report = {
                'job_info': {
                    'job_id': job_id,
                    'job_name': self.job_name,
                    'status': job_response.get('jobStatus'),
                    'created_at': job_response.get('createdAt'),
                    'completed_at': job_response.get('lastRunTime')
                },
                'statistics': job_response.get('statistics', {}),
                'findings_summary': {
                    'total_findings': len(findings_response.get('findingIds', [])),
                    'finding_ids': findings_response.get('findingIds', [])
                },
                'scan_scope': {
                    'bucket': self.scan_bucket,
                    'prefix': f"{self.s3_prefix}/{self.date_partition}/",
                    'date_partition': self.date_partition
                }
            }
            
            # å¦‚æœæœ‰å‘ç°ï¼Œè·å–è¯¦ç»†ä¿¡æ¯
            if analysis_report['findings_summary']['total_findings'] > 0:
                detailed_findings = []
                for finding_id in analysis_report['findings_summary']['finding_ids'][:10]:  # é™åˆ¶å‰10ä¸ª
                    try:
                        finding_detail = self.macie_client.get_findings(findingIds=[finding_id])
                        detailed_findings.append(finding_detail['findings'][0])
                    except Exception as e:
                        logger.warning(f"è·å–å‘ç°è¯¦æƒ…å¤±è´¥ {finding_id}: {e}")
                
                analysis_report['detailed_findings'] = detailed_findings
            
            # ä¿å­˜åˆ†ææŠ¥å‘Š
            report_filename = f"macie_analysis_report_{self.timestamp.strftime('%Y%m%d_%H%M%S')}.json"
            with open(report_filename, 'w', encoding='utf-8') as f:
                json.dump(analysis_report, f, indent=2, default=str, ensure_ascii=False)
            
            logger.info(f"âœ… åˆ†ææŠ¥å‘Šå·²ä¿å­˜: {report_filename}")
            
            # ä¸Šä¼ æŠ¥å‘Šåˆ°ç»“æœå­˜å‚¨æ¡¶
            try:
                results_key = f"loki-analysis/{self.date_partition}/{report_filename}"
                self.s3_client.upload_file(
                    report_filename,
                    self.results_bucket,
                    results_key
                )
                logger.info(f"âœ… æŠ¥å‘Šå·²ä¸Šä¼ åˆ°: s3://{self.results_bucket}/{results_key}")
            except Exception as e:
                logger.warning(f"ä¸Šä¼ æŠ¥å‘Šåˆ°ç»“æœå­˜å‚¨æ¡¶å¤±è´¥: {e}")
            
            return analysis_report
            
        except Exception as e:
            logger.error(f"åˆ†æMacieç»“æœå¤±è´¥: {e}")
            raise
    
    def run_complete_pipeline(self, chunk_dir: str = './lokichunk', output_dir: str = './extracted_texts'):
        """
        è¿è¡Œå®Œæ•´çš„åˆ†æç®¡é“
        """
        logger.info("ğŸš€ å¼€å§‹è¿è¡ŒLoki Macieåˆ†æç®¡é“")
        logger.info(f"Chunkç›®å½•: {chunk_dir}")
        logger.info(f"è¾“å‡ºç›®å½•: {output_dir}")
        logger.info(f"æ‰«æå­˜å‚¨æ¡¶: {self.scan_bucket}")
        logger.info(f"ç»“æœå­˜å‚¨æ¡¶: {self.results_bucket}")
        logger.info(f"æ—¶é—´åˆ†åŒº: {self.date_partition}")
        
        try:
            # æ­¥éª¤1: æå–Loki chunkæ–‡ä»¶ä¸ºæ–‡æœ¬
            logger.info("ğŸ“ æ­¥éª¤1: æå–Loki chunkæ–‡ä»¶")
            text_files = self.extract_loki_chunks_to_text(chunk_dir, output_dir)
            
            if not text_files:
                logger.error("âŒ æ²¡æœ‰æˆåŠŸæå–ä»»ä½•æ–‡ä»¶ï¼Œç»ˆæ­¢æµç¨‹")
                return None
            
            # æ­¥éª¤2: ä¸Šä¼ åˆ°S3
            logger.info("â˜ï¸ æ­¥éª¤2: ä¸Šä¼ æ–‡ä»¶åˆ°S3")
            uploaded_keys = self.upload_to_s3_with_partition(text_files)
            
            if not uploaded_keys:
                logger.error("âŒ æ²¡æœ‰æˆåŠŸä¸Šä¼ ä»»ä½•æ–‡ä»¶ï¼Œç»ˆæ­¢æµç¨‹")
                return None
            
            # æ­¥éª¤3: ç¡®ä¿Macieå·²å¯ç”¨
            logger.info("ğŸ” æ­¥éª¤3: æ£€æŸ¥MacieæœåŠ¡")
            if not self.ensure_macie_enabled():
                logger.error("âŒ MacieæœåŠ¡å¯ç”¨å¤±è´¥ï¼Œç»ˆæ­¢æµç¨‹")
                return None
            
            # æ­¥éª¤4: åˆ›å»ºMacieä½œä¸š
            logger.info("âš™ï¸ æ­¥éª¤4: åˆ›å»ºMacieåˆ†ç±»ä½œä¸š")
            job_id = self.create_macie_job(uploaded_keys)
            
            # ğŸ¯ å…³é”®å˜æ›´ï¼šè·å¾—job IDåç›´æ¥è¿”å›å‘½ä»¤è¡Œï¼Œä¸ç­‰å¾…å®Œæˆ
            logger.info("âœ… Macieä½œä¸šåˆ›å»ºæˆåŠŸï¼")
            
            # ç”Ÿæˆåˆ†æå‘½ä»¤
            analyze_command = f"python3 analyze_macie_results.py --job-id {job_id} --region {self.region}"
            if self.profile:
                analyze_command += f" --profile {self.profile}"
            
            # æ‰“å°ç»“æœæ‘˜è¦
            self.print_job_created_summary(job_id, analyze_command)
            
            return {
                'job_id': job_id,
                'analyze_command': analyze_command,
                'status': 'job_created'
            }
            
        except Exception as e:
            logger.error(f"âŒ ç®¡é“æ‰§è¡Œå¤±è´¥: {e}")
            raise
    
    def print_job_created_summary(self, job_id: str, analyze_command: str):
        """
        æ‰“å°ä½œä¸šåˆ›å»ºæˆåŠŸçš„æ‘˜è¦
        """
        print("\n" + "="*60)
        print("ğŸ‰ MACIE åˆ†æä½œä¸šå·²åˆ›å»ºæˆåŠŸï¼")
        print("="*60)
        
        print(f"ğŸ“‹ ä½œä¸šä¿¡æ¯:")
        print(f"   ä½œä¸šID: {job_id}")
        print(f"   ä½œä¸šåç§°: {self.job_name}")
        print(f"   åˆ›å»ºæ—¶é—´: {self.timestamp.isoformat()}")
        
        print(f"\nğŸ“ æ•°æ®ä½ç½®:")
        print(f"   æ‰«ææ•°æ®: s3://{self.scan_bucket}/{self.s3_prefix}/{self.date_partition}/")
        print(f"   ç»“æœå­˜å‚¨: s3://{self.results_bucket}/loki-analysis/{self.date_partition}/")
        
        print(f"\nâ³ ä½œä¸šçŠ¶æ€:")
        print(f"   Macieæ­£åœ¨åå°åˆ†ææ•°æ®...")
        print(f"   é¢„è®¡å®Œæˆæ—¶é—´: {self.config['macie']['max_wait_minutes']} åˆ†é’Ÿå†…")
        
        print(f"\nğŸ” æŸ¥çœ‹ç»“æœ:")
        print(f"   ç­‰å¾…å‡ åˆ†é’Ÿåè¿è¡Œä»¥ä¸‹å‘½ä»¤æŸ¥çœ‹åˆ†æç»“æœ:")
        print(f"   {analyze_command}")
        
        print(f"\nğŸ“Š ç›‘æ§ä½œä¸š:")
        print(f"   AWSæ§åˆ¶å°: https://{self.region}.console.aws.amazon.com/macie/home?region={self.region}#/jobs")
        
        print("="*60)
    
    def print_pipeline_summary(self, analysis_report: Dict):
        """
        æ‰“å°ç®¡é“æ‰§è¡Œæ‘˜è¦
        """
        print("\n" + "="*60)
        print("ğŸ¯ LOKI MACIE åˆ†æç®¡é“æ‰§è¡Œæ‘˜è¦")
        print("="*60)
        
        job_info = analysis_report.get('job_info', {})
        stats = analysis_report.get('statistics', {})
        findings = analysis_report.get('findings_summary', {})
        
        print(f"ğŸ“‹ ä½œä¸šä¿¡æ¯:")
        print(f"   ä½œä¸šID: {job_info.get('job_id')}")
        print(f"   ä½œä¸šåç§°: {job_info.get('job_name')}")
        print(f"   çŠ¶æ€: {job_info.get('status')}")
        print(f"   åˆ›å»ºæ—¶é—´: {job_info.get('created_at')}")
        
        print(f"\nğŸ“Š æ‰«æç»Ÿè®¡:")
        print(f"   å¤„ç†å¯¹è±¡æ•°: {stats.get('approximateNumberOfObjectsToProcess', 0)}")
        print(f"   å·²å¤„ç†å¯¹è±¡: {stats.get('approximateNumberOfObjectsProcessed', 0)}")
        
        print(f"\nğŸ” å‘ç°æ‘˜è¦:")
        print(f"   æ•æ„Ÿæ•°æ®å‘ç°æ•°: {findings.get('total_findings', 0)}")
        
        print(f"\nğŸ“ å­˜å‚¨ä½ç½®:")
        print(f"   æ‰«ææ•°æ®: s3://{self.scan_bucket}/{self.s3_prefix}/{self.date_partition}/")
        print(f"   åˆ†æç»“æœ: s3://{self.results_bucket}/loki-analysis/{self.date_partition}/")
        
        print("="*60)

def main():
    parser = argparse.ArgumentParser(description='Loki Chunkåˆ°AWS Macieå®Œæ•´åˆ†æç®¡é“')
    parser.add_argument('--chunk-dir', help='Loki chunkæ–‡ä»¶ç›®å½• (é»˜è®¤ä»é…ç½®æ–‡ä»¶è¯»å–)')
    parser.add_argument('--output-dir', help='æå–æ–‡æœ¬è¾“å‡ºç›®å½• (é»˜è®¤ä»é…ç½®æ–‡ä»¶è¯»å–)')
    parser.add_argument('--region', help='AWSåŒºåŸŸ (é»˜è®¤ä»é…ç½®æ–‡ä»¶è¯»å–)')
    parser.add_argument('--profile', help='AWSé…ç½®æ–‡ä»¶åç§° (é»˜è®¤ä»é…ç½®æ–‡ä»¶è¯»å–)')
    parser.add_argument('--max-wait', type=int, help='æœ€å¤§ç­‰å¾…æ—¶é—´(åˆ†é’Ÿ) (é»˜è®¤ä»é…ç½®æ–‡ä»¶è¯»å–)')
    parser.add_argument('--config', default='config.json', help='é…ç½®æ–‡ä»¶è·¯å¾„ (é»˜è®¤: config.json)')
    
    args = parser.parse_args()
    
    try:
        # åˆ›å»ºç®¡é“å®ä¾‹
        pipeline = LokiMaciePipeline(
            region=args.region, 
            profile=args.profile,
            config_file=args.config
        )
        
        # ä»é…ç½®æ–‡ä»¶æˆ–å‚æ•°è·å–è®¾ç½®
        chunk_dir = args.chunk_dir or pipeline.config['processing']['chunk_directory']
        output_dir = args.output_dir or pipeline.config['processing']['output_directory']
        
        # è¿è¡Œå®Œæ•´ç®¡é“
        result = pipeline.run_complete_pipeline(
            chunk_dir=chunk_dir,
            output_dir=output_dir
        )
        
        if result:
            print("\nâœ… ç®¡é“æ‰§è¡ŒæˆåŠŸå®Œæˆï¼")
            return 0
        else:
            print("\nâŒ ç®¡é“æ‰§è¡Œå¤±è´¥ï¼")
            return 1
            
    except Exception as e:
        logger.error(f"ç®¡é“æ‰§è¡Œå¼‚å¸¸: {e}")
        return 1

if __name__ == '__main__':
    exit(main())
